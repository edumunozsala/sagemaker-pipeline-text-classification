{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Sagemaker Pipeline to process data, train, evaluate and conditioning Batch Transform job\n",
    "\n",
    "## End-to-End pipeline in Sagemaker for a Keras Tensorflow Text Classifier\n",
    "\n",
    "In the last module of this workshop we will create a repeatable production workflow which is typically run outside notebooks. To demonstrate automating the workflow, we'll use [Amazon SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines) for workflow orchestration. Purpose-built for machine learning (ML), SageMaker Pipelines helps you automate different steps of the ML workflow including data processing, model training, and batch prediction (scoring), and apply conditions such as approvals for model quality. It also includes a model registry and model lineage tracker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Workflow Automation with SageMaker Pipelines <a class=\"anchor\" id=\"WorkflowAutomation\">\n",
    "\n",
    "In another notebook, we prototyped various steps of a TensorFlow project within the notebook itself, with the steps being run locally. Notebooks are great for prototyping, but generally are  not used in production-ready machine learning pipelines.  \n",
    "\n",
    "A very simple pipeline in SageMaker includes processing the dataset to get it ready for training, performing the actual training, and then using the model to perform some form of inference such as batch predition (scoring). We'll use SageMaker Pipelines to automate these steps, keeping the pipeline simple for now: it easily can be extended into a far more complex pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==2.91.1\n",
      "  Using cached sagemaker-2.91.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from sagemaker==2.91.1) (21.3)\n",
      "Requirement already satisfied: boto3>=1.20.21 in /usr/local/lib/python3.8/site-packages (from sagemaker==2.91.1) (1.20.22)\n",
      "Requirement already satisfied: protobuf>=3.1 in /usr/local/lib/python3.8/site-packages (from sagemaker==2.91.1) (3.19.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /usr/local/lib/python3.8/site-packages (from sagemaker==2.91.1) (1.0.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/site-packages (from sagemaker==2.91.1) (1.2.5)\n",
      "Collecting attrs==20.3.0\n",
      "  Using cached attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /usr/local/lib/python3.8/site-packages (from sagemaker==2.91.1) (0.1.5)\n",
      "Requirement already satisfied: google-pasta in /usr/local/lib/python3.8/site-packages (from sagemaker==2.91.1) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.8/site-packages (from sagemaker==2.91.1) (1.19.5)\n",
      "Requirement already satisfied: pathos in /usr/local/lib/python3.8/site-packages (from sagemaker==2.91.1) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /usr/local/lib/python3.8/site-packages (from sagemaker==2.91.1) (4.8.2)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.8/site-packages (from boto3>=1.20.21->sagemaker==2.91.1) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.8/site-packages (from boto3>=1.20.21->sagemaker==2.91.1) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.22 in /usr/local/lib/python3.8/site-packages (from boto3>=1.20.21->sagemaker==2.91.1) (1.23.22)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/site-packages (from importlib-metadata>=1.4.0->sagemaker==2.91.1) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from packaging>=20.0->sagemaker==2.91.1) (3.0.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker==2.91.1) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/site-packages (from pandas->sagemaker==2.91.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/site-packages (from pandas->sagemaker==2.91.1) (2021.3)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.8/site-packages (from pathos->sagemaker==2.91.1) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /usr/local/lib/python3.8/site-packages (from pathos->sagemaker==2.91.1) (0.3.0)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /usr/local/lib/python3.8/site-packages (from pathos->sagemaker==2.91.1) (0.70.12.2)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /usr/local/lib/python3.8/site-packages (from pathos->sagemaker==2.91.1) (1.6.6.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.8/site-packages (from botocore<1.24.0,>=1.23.22->boto3>=1.20.21->sagemaker==2.91.1) (1.25.11)\n",
      "Installing collected packages: attrs, sagemaker\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.2.0\n",
      "    Uninstalling attrs-21.2.0:\n",
      "      Successfully uninstalled attrs-21.2.0\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.70.0\n",
      "    Uninstalling sagemaker-2.70.0:\n",
      "      Successfully uninstalled sagemaker-2.70.0\n",
      "Successfully installed attrs-20.3.0 sagemaker-2.91.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker=='2.91.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting global variables and Sagemaker session parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket() \n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Set the dataset folder\n",
    "raw_s3 = f\"s3://{bucket}/keras-text-classification/data/raw\"    # sess.upload_data(path='./data/raw/', key_prefix=rawdata_s3_prefix)\n",
    "# Set the dataset for batch predictions\n",
    "batch_s3 = f\"s3://{bucket}/keras-text-classification/data/batch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: sagemaker-us-east-1-223817798831\n",
      "Region: us-east-1\n",
      "Folder: s3://sagemaker-us-east-1-223817798831/keras-text-classification/data/raw\n",
      "Batch Folder: s3://sagemaker-us-east-1-223817798831/keras-text-classification/data/batch\n",
      "Role: arn:aws:iam::223817798831:role/service-role/AmazonSageMaker-ExecutionRole-20200708T194212\n"
     ]
    }
   ],
   "source": [
    "print('Bucket:',bucket)\n",
    "print('Region:',region)\n",
    "print('Folder:',raw_s3)\n",
    "print('Batch Folder:',batch_s3)\n",
    "print('Role:', role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload a dataset for batch transformation after model creation. You can select your own dataset for the batch_data_uri as is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-223817798831/keras-text-classification/data/batch\n"
     ]
    }
   ],
   "source": [
    "####### In this notebook we are not using this dataset  ###############################\n",
    "local_path = \"data/batch\"\n",
    "# Upload the dataset for batch transformation\n",
    "batch_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,\n",
    "    desired_s3_uri=batch_s3,\n",
    ")\n",
    "print(batch_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Pipeline parameters <a class=\"anchor\" id=\"PipelineParameters\">\n",
    "\n",
    "Before we begin to create the pipeline itself, we should think about how to parameterize it.  For example, we may use different instance types for different purposes, such as CPU-based types for data processing and GPU-based or more powerful types for model training.  These are all \"knobs\" of the pipeline that we can parameterize.  Parameterizing enables custom pipeline executions and schedules without having to modify the pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "# raw input data\n",
    "#input_data = ParameterString(name=\"InputData\", default_value=raw_s3)\n",
    "\n",
    "# processing step parameters\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.t3.xlarge\") #\"ml.m5.xlarge\" o c5.xlarge\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "# training step parameters\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.c5.xlarge\")\n",
    "training_instance_count = ParameterInteger(name=\"TrainingInstanceCount\", default_value=1)\n",
    "\n",
    "# batch inference step parameters\n",
    "batch_instance_type = ParameterString(name=\"BatchInstanceType\", default_value=\"ml.c5.xlarge\")\n",
    "batch_instance_count = ParameterInteger(name=\"BatchInstanceCount\", default_value=1)\n",
    "# Inference step parameters\n",
    "endpoint_instance_type = ParameterString(name=\"EndpointInstanceType\", default_value=\"ml.m5.large\")\n",
    "\n",
    "# Model approval parameter\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"Approved\"\n",
    ")\n",
    "# Accuracy Threshold\n",
    "acc_threshold = ParameterFloat(name=\"AccThreshold\", default_value=0.8)\n",
    "# dataset for batch transformation\n",
    "batch_data = ParameterString(\n",
    "    name=\"BatchData\",\n",
    "    default_value=batch_data_uri,\n",
    ")\n",
    "\n",
    "# training step parameters\n",
    "training_epochs = ParameterString(name=\"TrainingEpochs\", default_value=\"5\")\n",
    "max_vocab = ParameterString(name=\"MaxVocab\", default_value=\"10000\")\n",
    "max_length = ParameterString(name=\"MaxLength\", default_value=\"250\")\n",
    "\n",
    "# Project Name\n",
    "project_name = 'keras-text-classification'\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching Configuration\n",
    "\n",
    "This notebook demonstrates how to take advantage of pipeline step caching. With step caching, SageMaker tracks the arguments used for each step execution and re-uses previous, successful executions when the call signatures match. SageMaker only tracks arguments important for the output of the step, so pipeline steps are optimized for cache hits and unnecessary step executions are avoided.\n",
    "\n",
    "Executing the step without changing its configurations, inputs, or outputs can be a waste. Thus, we can enable caching for pipeline steps. When caching is enabled, an expiration time (in ISO8601 duration string format) needs to be supplied. The expiration time indicates how old a previous execution can be to be considered for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(\n",
    "    enable_caching=True,\n",
    "    expire_after=\"P7d\" # 7-day\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Processing Step <a class=\"anchor\" id=\"ProcessingStep\">\n",
    "\n",
    "The first step in the pipeline will preprocess the data to prepare it for training. We create a `SKLearnProcessor` object, parameterized so we can separately track and change the job configuration as needed, for example to increase the instance type size and count to accommodate a growing dataset.\n",
    "    \n",
    "Previously we have created a preprocessing scriptfile, which contains the preprocessing tasks:\n",
    "    - Download the datafiles from a URL\n",
    "    - Create the train, validation and test datasets\n",
    "\n",
    "Then we create an instance of a SKLearnProcessor processor and use that in our ProcessingStep. You also specify the framework_version to use throughout this notebook.\n",
    "Note the processing_instance_type and processing_instance_count parameter used by the processor instance.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"ktc-workflow-process\",\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    "    max_runtime_in_seconds=1800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting tha parameters of the data processing stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "datapath = \"aclImdb_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"KTCProcess\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=None, #[ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\", s3_data_distribution_type='ShardedByS3Key'),],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", s3_upload_mode='EndOfJob'),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\",s3_upload_mode='EndOfJob'),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\", s3_upload_mode='EndOfJob'),\n",
    "    ],\n",
    "    code=\"scripts/preprocess.py\",\n",
    "    job_arguments=['--url', url,                   \n",
    "                   '--datapath', datapath,\n",
    "                   #'--feature-group-name', str(feature_group_name.default_value)\n",
    "                  ],\n",
    "    cache_config=cache_config\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define a Training Step to Train a Keras model\n",
    "### Model building and Training Step <a class=\"anchor\" id=\"TrainingModelCreation\">\n",
    "\n",
    "The following code sets up a pipeline step for a training job. We start by specifying which SageMaker prebuilt TensorFlow 2 training container to use for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "tensorflow_version = '2.6.3'\n",
    "python_version = 'py38'\n",
    "\n",
    "image_uri_train = sagemaker.image_uris.retrieve(\n",
    "                                        framework=\"tensorflow\",\n",
    "                                        region=region,\n",
    "                                        version=tensorflow_version,\n",
    "                                        py_version=python_version,\n",
    "                                        instance_type=training_instance_type,\n",
    "                                        image_scope=\"training\"\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Next, we specify an `Estimator` object, and define a `TrainingStep` to insert the training job in the pipeline with inputs from the previous SageMaker Processing step. We should use it to find the best model.\n",
    "\n",
    "Configure an Estimator for a Tensorflow model and the output path. A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later in the output_path defined.\n",
    "\n",
    "Note the training_instance_type and training_instance_count parameters are used to train in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Set the path to the trained model\n",
    "model_path = f\"s3://{bucket}/keras-text-classification/model\"\n",
    "training_parameters = {'epochs': training_epochs, 'max-vocab': max_vocab, 'max-length': max_length}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    image_uri=image_uri_train,\n",
    "    source_dir='scripts',\n",
    "    entry_point='train.py',\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=training_instance_count,\n",
    "    role=role,\n",
    "    base_job_name=\"keras-classifier-train\",\n",
    "    output_path=model_path,\n",
    "    hyperparameters=training_parameters,\n",
    "    disable_profiler= True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "step_train = TrainingStep(\n",
    "    name=\"KTCWorkflowTrain\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "        \n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri\n",
    "        )\n",
    "    },\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define a Model Evaluation Step to Evaluate the Trained Model\n",
    "\n",
    "We have developed an evaluation script that is specified in a Processing step that performs the model evaluation. After pipeline execution, you can examine the resulting evaluation.json for analysis.\n",
    "\n",
    "The evaluation script uses the trained model to do the following:\n",
    "\n",
    "- Load the model.\n",
    "- Read the test dataset\n",
    "- Launch predictions against the test data.\n",
    "- Calculate the accuracy on the test data.\n",
    "- Save the result to the evaluation directory as a json file.\n",
    "\n",
    "As another step, we create a SageMaker `Model` object to wrap the model artifact, and associate it with a separate SageMaker prebuilt TensorFlow Serving inference container to potentially use later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an instance of a `SKLearnProcessor` and use it in the ProcessingStep.\n",
    "\n",
    "Use the processor's arguments to construct a ProcessingStep, along with the input and output channels and the code that will be executed when the pipeline invokes pipeline execution. Specifically, the S3ModelArtifacts from the step_train properties and the S3Uri of the \"test_data\" output channel of the step_process properties are passed as inputs. The TrainingStep and ProcessingStep properties attribute matches the object model of the DescribeTrainingJob and DescribeProcessingJob response objects, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "# Create the processor\n",
    "evaluation_scorer = SKLearnProcessor(\n",
    "                    framework_version=framework_version,\n",
    "                    instance_type=processing_instance_type,\n",
    "                    instance_count=processing_instance_count,\n",
    "                    base_job_name=\"KTC-workflow-evaluation\",\n",
    "                    sagemaker_session=sess,\n",
    "                    role=role )\n",
    "\n",
    "# Create a PropertyFile\n",
    "# A PropertyFile is used to be able to reference outputs from a processing step, for instance to use in a condition step.\n",
    "# For more information, visit https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-propertyfile.html\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "# Create the Evaluation Step\n",
    "step_evaluation = ProcessingStep(\n",
    "                    name=\"KTCWorkflowEvaluation\",\n",
    "                    processor=evaluation_scorer,\n",
    "                    inputs=[\n",
    "                        ProcessingInput(\n",
    "                            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                            destination=\"/opt/ml/processing/model\"\n",
    "                        ),\n",
    "                        ProcessingInput(\n",
    "                            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                                \"test\"\n",
    "                            ].S3Output.S3Uri,\n",
    "                            destination=\"/opt/ml/processing/test\"\n",
    "                        )\n",
    "                    ],\n",
    "                    outputs=[\n",
    "                        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "                    ],\n",
    "                    code=\"./scripts/evaluation.py\",\n",
    "                    property_files=[evaluation_report],\n",
    "                    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Register Model Step \n",
    "\n",
    "A model package is an abstraction of reusable model artifacts that packages all ingredients required for inference. Primarily, it consists of an inference specification that defines the inference image to use along with an optional model weights location.\n",
    "\n",
    "A model package group is a collection of model packages. A model package group can be created for a specific ML business problem, and new versions of the model packages can be added to it. Typically, customers are expected to create a ModelPackageGroup for a SageMaker pipeline so that model package versions can be added to the group for every SageMaker Pipeline run.\n",
    "\n",
    "To register a model in the Model Registry, we take the model created in the previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class RegisterModel has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "model_package_group_name = f\"KTCModelPackageGroupName\"\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_evaluation.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "step_register = RegisterModel(\n",
    "    name=\"KTCRegisterModel\",\n",
    "    estimator=estimator,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.c5.xlarge\"], #\"ml.t2.xlarge\"\n",
    "    transform_instances=[\"ml.c5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    #approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "#step_register = ModelStep(name=\"KTCRegisterModel\", step_args=register_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Create Model Step to Create a Model\n",
    "\n",
    "In order to perform predictions using the model, we create a SageMaker model. Specifically, pass in the S3ModelArtifacts from the TrainingStep, step_train properties. The TrainingStep properties attribute matches the object model of the DescribeTrainingJob response object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another step, we create a SageMaker `Model` object to wrap the model artifact, and associate it with a separate SageMaker prebuilt TensorFlow Serving inference container to potentially use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "#from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "# Get the right image URI for a tensorflow model\n",
    "image_uri_inference = sagemaker.image_uris.retrieve(\n",
    "                                        framework=\"tensorflow\",\n",
    "                                        region=region,\n",
    "                                        version=tensorflow_version,\n",
    "                                        py_version=python_version,\n",
    "                                        instance_type=batch_instance_type,\n",
    "                                        image_scope=\"inference\"\n",
    "                                       )\n",
    "# Create the model\n",
    "model = Model(\n",
    "    image_uri=image_uri_inference,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "inputs_model = CreateModelInput(\n",
    "    instance_type=batch_instance_type\n",
    ")\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"KTCWorkflowCreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs_model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO BE REMOVED IN FUTURE REALESES\n",
    "## Define a Transform Step to Perform Batch Transformation\n",
    "\n",
    "Now that a model instance is defined, create a Transformer instance with the appropriate model type, compute instance type, and desired output S3 URI.\n",
    "\n",
    "Specifically, pass in the ModelName from the CreateModelStep, step_create_model properties. The CreateModelStep properties attribute matches the object model of the DescribeModel response object.\n",
    "\n",
    "Pass in the transformer instance and the TransformInput with the batch_data pipeline parameter defined earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=batch_instance_type,\n",
    "    instance_count=batch_instance_count,\n",
    "    output_path=f\"s3://{bucket}/KTCWorkflowTransform\",\n",
    ")\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"KTCWorkflowTransform\", transformer=transformer, inputs=TransformInput(data=batch_data),\n",
    "    #cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model to SageMaker Endpoint Lambda Step\n",
    "\n",
    "When defining the LambdaStep, the SageMaker Lambda helper class provides helper functions for creating the Lambda function. Users can either use the lambda_func argument to provide the function ARN to an already deployed Lambda function OR use the Lambda class to create a Lambda function by providing a script, function name and role for the Lambda function.\n",
    "\n",
    "When passing inputs to the Lambda, the inputs argument can be used and within the Lambda function's handler, the event argument can be used to retrieve the inputs.\n",
    "\n",
    "The dictionary response from the Lambda function is parsed through the LambdaOutput objects provided to the outputs argument. The output_name in LambdaOutput corresponds to the dictionary key in the Lambda's return dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IAM Role**\n",
    "\n",
    "The Lambda function needs an IAM role that will allow it to deploy a SageMaker Endpoint. The role ARN must be provided in the LambdaStep.\n",
    "\n",
    "A helper function in iam_helper.py is available to create the Lambda function role. Please note that the role uses the Amazon managed policy - AmazonSageMakerFullAccess. This should be replaced with an IAM policy with the least privileges as per AWS IAM best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 30 seconds for the IAM role to propagate\n"
     ]
    }
   ],
   "source": [
    "from iam_helper import create_sagemaker_lambda_role\n",
    "from sagemaker.workflow.lambda_step import LambdaStep\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "\n",
    "lambda_role = create_sagemaker_lambda_role(\"deploy-model-lambda-role\")\n",
    "\n",
    "endpoint_config_name = project_name+ \"-endpoint-config\"\n",
    "endpoint_name = project_name+ \"-endpoint-\" + current_time\n",
    "\n",
    "deploy_model_lambda_function_name = \"sagemaker-deploy-model-lambda-\" + current_time\n",
    "\n",
    "deploy_model_lambda_function = Lambda(\n",
    "    function_name=deploy_model_lambda_function_name,\n",
    "    execution_role_arn=lambda_role,\n",
    "    script=\"./scripts/deploy_model_lambda.py\",\n",
    "    handler=\"deploy_model_lambda.lambda_handler\",\n",
    ")\n",
    "\n",
    "step_deploy_model_lambda = LambdaStep(\n",
    "    name=\"KTCWorkflowModelEndpoint\",\n",
    "    lambda_func=deploy_model_lambda_function,\n",
    "    inputs={\n",
    "        \"model_name\": step_create_model.properties.ModelName,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"endpoint_instance_type\": endpoint_instance_type,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Fail Step to Terminate the Pipeline Execution and Mark it as Failed\n",
    "\n",
    "Define a FailStep with customized error message, which indicates the cause of the execution failure.\n",
    "Enter the FailStep error message with a Join function, which appends a static text string with the dynamic mse_threshold parameter to build a more informative error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "step_fail = FailStep(\n",
    "    name=\"KTCWorkflowACCFail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to ACC <\", acc_threshold]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Condition Step to Check Accuracy and Conditionally Create a Model and Run a Batch Transformation Or Terminate the Execution in Failed State\n",
    "\n",
    "In this step, the model is registered only if the accuracy of the model, as determined by the evaluation step step_eval, exceeded a specified value. Otherwise, the pipeline execution fails and terminates. A ConditionStep enables pipelines to support conditional execution in the pipeline DAG based on the conditions of the step properties.\n",
    "\n",
    "In the following section, you:\n",
    "\n",
    "Define a ConditionGreaterThan on the accuracy value found in the output of the evaluation step, step_eval.\n",
    "Use the condition in the list of conditions in a ConditionStep.\n",
    "Pass the CreateModelStep and TransformStep steps, and the RegisterModel step collection into the if_steps of the ConditionStep, which are only executed if the condition evaluates to True.\n",
    "Pass the FailStep step into the else_stepsof the ConditionStep, which is only executed if the condition evaluates to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThan\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "\n",
    "cond_lte = ConditionGreaterThan(\n",
    "    left=JsonGet(\n",
    "        step_name=step_evaluation.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"classification_metrics.accuracy.value\",\n",
    "    ),\n",
    "    right=acc_threshold,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"KTCWorkflowACCCond\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register, step_create_model, step_deploy_model_lambda], #, step_transform\n",
    "    else_steps=[step_fail],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define a Pipeline of Parameters, Steps, and Conditions\n",
    "In this section, combine the steps into a Pipeline so it can be executed.\n",
    "\n",
    "A pipeline requires a name, parameters, and steps. Names must be unique within an (account, region) pair.\n",
    "\n",
    "Note:\n",
    "\n",
    "All the parameters used in the definitions must be present.\n",
    "Steps passed into the pipeline do not have to be listed in the order of execution. The SageMaker Pipeline service resolves the data dependency DAG as steps for the execution to complete.\n",
    "Steps must be unique to across the pipeline step list and all condition step if/else lists.\n",
    "\n",
    "\n",
    "With all of the pipeline steps now defined, we can define the pipeline itself as a `Pipeline` object comprising a series of those steps.  Parallel and steps also are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"KTCWorkflow\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[#input_data,\n",
    "                processing_instance_type, \n",
    "                processing_instance_count, \n",
    "                training_instance_type, \n",
    "                training_instance_count,\n",
    "                batch_instance_type,\n",
    "                batch_instance_count,\n",
    "                endpoint_instance_type,\n",
    "                batch_data,\n",
    "                #model_approval_status,\n",
    "                training_epochs,\n",
    "                max_vocab,\n",
    "                max_length,\n",
    "                acc_threshold],\n",
    "    steps=[step_process, \n",
    "           step_train, \n",
    "           step_evaluation,\n",
    "           step_cond\n",
    "          ],\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We can inspect the pipeline definition in JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.t3.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.c5.xlarge'},\n",
       "  {'Name': 'TrainingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'BatchInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.c5.xlarge'},\n",
       "  {'Name': 'BatchInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'EndpointInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.large'},\n",
       "  {'Name': 'BatchData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-east-1-223817798831/keras-text-classification/data/batch'},\n",
       "  {'Name': 'TrainingEpochs', 'Type': 'String', 'DefaultValue': '5'},\n",
       "  {'Name': 'MaxVocab', 'Type': 'String', 'DefaultValue': '10000'},\n",
       "  {'Name': 'MaxLength', 'Type': 'String', 'DefaultValue': '250'},\n",
       "  {'Name': 'AccThreshold', 'Type': 'Float', 'DefaultValue': 0.8}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'KTCProcess',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerArguments': ['--url',\n",
       "      'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n",
       "      '--datapath',\n",
       "      'aclImdb_v1'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocess.py']},\n",
       "    'RoleArn': 'arn:aws:iam::223817798831:role/service-role/AmazonSageMaker-ExecutionRole-20200708T194212',\n",
       "    'ProcessingInputs': [{'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-223817798831/KTCProcess-0a3e96c71c9090994788a5036ca51347/input/code/preprocess.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-223817798831/KTCProcess-0a3e96c71c9090994788a5036ca51347/output/train',\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'validation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-223817798831/KTCProcess-0a3e96c71c9090994788a5036ca51347/output/validation',\n",
       "        'LocalPath': '/opt/ml/processing/validation',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-223817798831/KTCProcess-0a3e96c71c9090994788a5036ca51347/output/test',\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 1800}},\n",
       "   'CacheConfig': {'Enabled': True, 'ExpireAfter': 'P7d'}},\n",
       "  {'Name': 'KTCWorkflowTrain',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.6.3-cpu-py38'},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-223817798831/keras-text-classification/model'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'InstanceCount': {'Get': 'Parameters.TrainingInstanceCount'},\n",
       "     'InstanceType': {'Get': 'Parameters.TrainingInstanceType'},\n",
       "     'VolumeSizeInGB': 30},\n",
       "    'RoleArn': 'arn:aws:iam::223817798831:role/service-role/AmazonSageMaker-ExecutionRole-20200708T194212',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.KTCProcess.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.KTCProcess.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'validation'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.KTCProcess.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'test'}],\n",
       "    'HyperParameters': {'epochs': {'Get': 'Parameters.TrainingEpochs'},\n",
       "     'max-vocab': {'Get': 'Parameters.MaxVocab'},\n",
       "     'max-length': {'Get': 'Parameters.MaxLength'},\n",
       "     'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-223817798831/KTCWorkflowTrain-a3ac9d58f802a8cf62f4ebd986d0426f/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"train.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_region': '\"us-east-1\"',\n",
       "     'model_dir': '\"s3://sagemaker-us-east-1-223817798831/keras-text-classification/model/KTCWorkflowTrain-a3ac9d58f802a8cf62f4ebd986d0426f/model\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-223817798831/keras-text-classification/model',\n",
       "     'CollectionConfigurations': []}},\n",
       "   'CacheConfig': {'Enabled': True, 'ExpireAfter': 'P7d'}},\n",
       "  {'Name': 'KTCWorkflowEvaluation',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/evaluation.py']},\n",
       "    'RoleArn': 'arn:aws:iam::223817798831:role/service-role/AmazonSageMaker-ExecutionRole-20200708T194212',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Steps.KTCWorkflowTrain.ModelArtifacts.S3ModelArtifacts'},\n",
       "       'LocalPath': '/opt/ml/processing/model',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'input-2',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.KTCProcess.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/test',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-223817798831/KTCWorkflowEvaluation-86a4cbc847f1dc8f95b265506c24256c/input/code/evaluation.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'evaluation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-223817798831/KTCWorkflowEvaluation-86a4cbc847f1dc8f95b265506c24256c/output/evaluation',\n",
       "        'LocalPath': '/opt/ml/processing/evaluation',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'CacheConfig': {'Enabled': True, 'ExpireAfter': 'P7d'},\n",
       "   'PropertyFiles': [{'PropertyFileName': 'EvaluationReport',\n",
       "     'OutputName': 'evaluation',\n",
       "     'FilePath': 'evaluation.json'}]},\n",
       "  {'Name': 'KTCWorkflowACCCond',\n",
       "   'Type': 'Condition',\n",
       "   'Arguments': {'Conditions': [{'Type': 'GreaterThan',\n",
       "      'LeftValue': {'Std:JsonGet': {'PropertyFile': {'Get': 'Steps.KTCWorkflowEvaluation.PropertyFiles.EvaluationReport'},\n",
       "        'Path': 'classification_metrics.accuracy.value'}},\n",
       "      'RightValue': {'Get': 'Parameters.AccThreshold'}}],\n",
       "    'IfSteps': [{'Name': 'KTCRegisterModel',\n",
       "      'Type': 'RegisterModel',\n",
       "      'Arguments': {'ModelPackageGroupName': 'KTCModelPackageGroupName',\n",
       "       'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "          'S3Uri': 's3://sagemaker-us-east-1-223817798831/KTCWorkflowEvaluation-86a4cbc847f1dc8f95b265506c24256c/output/evaluation/evaluation.json'}},\n",
       "        'Bias': {},\n",
       "        'Explainability': {}},\n",
       "       'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.6.3-cpu-py38',\n",
       "          'ModelDataUrl': {'Get': 'Steps.KTCWorkflowTrain.ModelArtifacts.S3ModelArtifacts'}}],\n",
       "        'SupportedContentTypes': ['text/csv'],\n",
       "        'SupportedResponseMIMETypes': ['text/csv'],\n",
       "        'SupportedRealtimeInferenceInstanceTypes': ['ml.c5.xlarge'],\n",
       "        'SupportedTransformInstanceTypes': ['ml.c5.xlarge']},\n",
       "       'ModelApprovalStatus': 'PendingManualApproval'}},\n",
       "     {'Name': 'KTCWorkflowCreateModel',\n",
       "      'Type': 'Model',\n",
       "      'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::223817798831:role/service-role/AmazonSageMaker-ExecutionRole-20200708T194212',\n",
       "       'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.6.3-cpu',\n",
       "        'Environment': {},\n",
       "        'ModelDataUrl': {'Get': 'Steps.KTCWorkflowTrain.ModelArtifacts.S3ModelArtifacts'}}}},\n",
       "     {'Name': 'KTCWorkflowModelEndpoint',\n",
       "      'Type': 'Lambda',\n",
       "      'Arguments': {'model_name': {'Get': 'Steps.KTCWorkflowCreateModel.ModelName'},\n",
       "       'endpoint_config_name': 'keras-text-classification-endpoint-config',\n",
       "       'endpoint_name': 'keras-text-classification-endpoint-11-09-11-06-45',\n",
       "       'endpoint_instance_type': {'Get': 'Parameters.EndpointInstanceType'}},\n",
       "      'FunctionArn': 'arn:aws:lambda:us-east-1:223817798831:function:sagemaker-deploy-model-lambda-11-09-11-06-45',\n",
       "      'OutputParameters': []}],\n",
       "    'ElseSteps': [{'Name': 'KTCWorkflowACCFail',\n",
       "      'Type': 'Fail',\n",
       "      'Arguments': {'ErrorMessage': {'Std:Join': {'On': ' ',\n",
       "         'Values': ['Execution failed due to ACC <',\n",
       "          {'Get': 'Parameters.AccThreshold'}]}}}}]}}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "After upserting its definition, we can start the pipeline with the `Pipeline` object's `start` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We can now confirm that the pipeline is executing.  In the log output below, confirm that `PipelineExecutionStatus` is `Executing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:223817798831:pipeline/ktcworkflow',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:223817798831:pipeline/ktcworkflow/execution/kel3avcgfvih',\n",
       " 'PipelineExecutionDisplayName': 'execution-1667992835096',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'ktcworkflow',\n",
       "  'TrialName': 'kel3avcgfvih'},\n",
       " 'CreationTime': datetime.datetime(2022, 11, 9, 11, 20, 35, 9000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 11, 9, 11, 20, 35, 9000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:223817798831:user-profile/d-apj1jigbprcn/sagemakeruser',\n",
       "  'UserProfileName': 'sagemakeruser',\n",
       "  'DomainId': 'd-apj1jigbprcn'},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:223817798831:user-profile/d-apj1jigbprcn/sagemakeruser',\n",
       "  'UserProfileName': 'sagemakeruser',\n",
       "  'DomainId': 'd-apj1jigbprcn'},\n",
       " 'ResponseMetadata': {'RequestId': '1f3fa859-708f-4dad-a0fa-ec48106a9d64',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1f3fa859-708f-4dad-a0fa-ec48106a9d64',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '800',\n",
       "   'date': 'Wed, 09 Nov 2022 11:21:14 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the Pipeline\n",
    "\n",
    "After the pipeline started executing, you can view the pipeline run. \n",
    "\n",
    "To view them, choose the SageMakers Components and registries button.\n",
    "On the Components and registires drop down, select Pipelines.\n",
    "\n",
    "\n",
    "Click the `KTCWorkflow` pipeline, and then double click on the execution.\n",
    "\n",
    "\n",
    "Now you can see the pipeline executing. Click on `KTCProcess` step to see additional details.\n",
    "\n",
    "\n",
    "On this specific step, you'll be able to see the output, logs and information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Typically this pipeline should take about 15-18 minutes to complete.  We can wait for completion by invoking `wait()`. After execution is complete, we can list the status of the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'TF2WorkflowCreateModel',\n",
       "  'StartTime': datetime.datetime(2022, 8, 26, 16, 15, 59, 673000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 8, 26, 16, 16, 0, 997000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:223817798831:model/pipelines-eu668p7gi5we-tf2workflowcreatemod-sku5clkcl9'}}},\n",
       " {'StepName': 'TF2WorkflowBatchScoring',\n",
       "  'StartTime': datetime.datetime(2022, 8, 26, 16, 15, 59, 673000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 8, 26, 16, 20, 53, 514000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:223817798831:processing-job/pipelines-eu668p7gi5we-tf2workflowbatchscor-r6oyk1l75i'}}},\n",
       " {'StepName': 'TF2WorkflowTrain',\n",
       "  'StartTime': datetime.datetime(2022, 8, 26, 16, 12, 27, 405000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 8, 26, 16, 15, 58, 487000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:223817798831:training-job/pipelines-eu668p7gi5we-tf2workflowtrain-hbg8wkvbg0'}}},\n",
       " {'StepName': 'TF2Process',\n",
       "  'StartTime': datetime.datetime(2022, 8, 26, 16, 8, 4, 252000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 8, 26, 16, 12, 26, 695000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:223817798831:processing-job/pipelines-eu668p7gi5we-tf2process-vajndycyvp'}}}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'KTCWorkflowModelEndpoint',\n",
       "  'StartTime': datetime.datetime(2022, 11, 9, 11, 35, 3, 644000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 11, 9, 11, 35, 5, 719000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'Lambda': {'Arn': 'arn:aws:lambda:us-east-1:223817798831:function:sagemaker-deploy-model-lambda-11-09-11-06-45',\n",
       "    'OutputParameters': [{'Name': 'body',\n",
       "      'Value': '\"Endpoint Created Successfully\"'},\n",
       "     {'Name': 'statusCode', 'Value': '200.0'}]}}},\n",
       " {'StepName': 'KTCRegisterModel',\n",
       "  'StartTime': datetime.datetime(2022, 11, 9, 11, 35, 1, 944000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 11, 9, 11, 35, 2, 896000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:223817798831:model-package/ktcmodelpackagegroupname/8'}}},\n",
       " {'StepName': 'KTCWorkflowCreateModel',\n",
       "  'StartTime': datetime.datetime(2022, 11, 9, 11, 35, 1, 944000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 11, 9, 11, 35, 3, 109000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:223817798831:model/pipelines-kel3avcgfvih-ktcworkflowcreatemod-ikjaliitqr'}}},\n",
       " {'StepName': 'KTCWorkflowACCCond',\n",
       "  'StartTime': datetime.datetime(2022, 11, 9, 11, 35, 0, 898000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 11, 9, 11, 35, 1, 344000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'Condition': {'Outcome': 'True'}}},\n",
       " {'StepName': 'KTCWorkflowEvaluation',\n",
       "  'StartTime': datetime.datetime(2022, 11, 9, 11, 29, 52, 54000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 11, 9, 11, 35, 0, 329000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:223817798831:processing-job/pipelines-kel3avcgfvih-ktcworkflowevaluatio-0wysiuv8iy'}}},\n",
       " {'StepName': 'KTCWorkflowTrain',\n",
       "  'StartTime': datetime.datetime(2022, 11, 9, 11, 26, 9, 368000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 11, 9, 11, 29, 50, 586000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:223817798831:training-job/pipelines-kel3avcgfvih-ktcworkflowtrain-dx8ngdrjoz'}}},\n",
       " {'StepName': 'KTCProcess',\n",
       "  'StartTime': datetime.datetime(2022, 11, 9, 11, 20, 36, 333000, tzinfo=tzlocal()),\n",
       "  'EndTime': datetime.datetime(2022, 11, 9, 11, 26, 8, 531000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Succeeded',\n",
       "  'AttemptCount': 0,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:223817798831:processing-job/pipelines-kel3avcgfvih-ktcprocess-dqm3qxchze'}}}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Check the score report\n",
    "\n",
    "After the evaluation step in the pipeline is complete, the test scoring report is uploaded to S3.  For simplicity, this report simply states the test Accuracy, but in general reports can include as much detail as desired.  This reports can be use in conditional approval steps in SageMaker Pipelines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification_metrics': {'accuracy': {'value': 0.872160017490387}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\n",
    "    \"{}/evaluation.json\".format(\n",
    "        step_evaluation.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "    )\n",
    ")\n",
    "pprint(json.loads(evaluation_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## ML Lineage Tracking <a class=\"anchor\" id=\"LineageOfPipelineArtifacts\">\n",
    "\n",
    "SageMaker ML Lineage Tracking creates and stores information about the steps of a ML workflow from data preparation to model deployment. With the tracking information you can reproduce the workflow steps, track model and dataset lineage, and establish model governance and audit standards.\n",
    "\n",
    "Let's now check out the lineage of the model generated by the pipeline above.  The lineage table identifies the resources used in training, including the timestamped train and test data sources, and the specific version of the TensorFlow 2 container in use during the training job.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...96c71c9090994788a5036ca51347/output/test</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...9090994788a5036ca51347/output/validation</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...6c71c9090994788a5036ca51347/output/train</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76310...s.com/tensorflow-training:2.6.3-cpu-py38</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...flowTrain-dx8nGDrjOZ/output/model.tar.gz</td>\n",
       "      <td>Output</td>\n",
       "      <td>Model</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...96c71c9090994788a5036ca51347/output/test     Input  DataSet   \n",
       "1  s3://...9090994788a5036ca51347/output/validation     Input  DataSet   \n",
       "2  s3://...6c71c9090994788a5036ca51347/output/train     Input  DataSet   \n",
       "3  76310...s.com/tensorflow-training:2.6.3-cpu-py38     Input    Image   \n",
       "4  s3://...flowTrain-dx8nGDrjOZ/output/model.tar.gz    Output    Model   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo     artifact  \n",
       "4         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    if execution_step['StepName'] == 'KTCWorkflowTrain':\n",
    "        display(viz.show(pipeline_execution_step=execution_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up (optional)\n",
    "\n",
    "**Stop / Close the Endpoint**\n",
    "\n",
    "You should delete the endpoint before you close the notebook if you don't need to keep the endpoint running for serving real-time predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint:  keras-text-classification-endpoint-11-09-11-06-45\n"
     ]
    }
   ],
   "source": [
    "print('Endpoint: ',endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '61fa3769-544b-416f-9542-453078ae423b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '61fa3769-544b-416f-9542-453078ae423b',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 09 Nov 2022 11:40:48 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client(\"sagemaker\")\n",
    "client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the model registry and the pipeline to keep the studio environment tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_model_package_group(sm_client, package_group_name):\n",
    "    try:\n",
    "        model_versions = sm_client.list_model_packages(ModelPackageGroupName=package_group_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "        return\n",
    "\n",
    "    for model_version in model_versions[\"ModelPackageSummaryList\"]:\n",
    "        try:\n",
    "            sm_client.delete_model_package(ModelPackageName=model_version[\"ModelPackageArn\"])\n",
    "        except Exception as e:\n",
    "            print(\"{} \\n\".format(e))\n",
    "        time.sleep(0.5)  # Ensure requests aren't throttled\n",
    "\n",
    "    try:\n",
    "        sm_client.delete_model_package_group(ModelPackageGroupName=package_group_name)\n",
    "        print(\"{} model package group deleted\".format(package_group_name))\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "    return\n",
    "\n",
    "\n",
    "def delete_sagemaker_pipeline(sm_client, pipeline_name):\n",
    "    try:\n",
    "        sm_client.delete_pipeline(\n",
    "            PipelineName=pipeline_name,\n",
    "        )\n",
    "        print(\"{} pipeline deleted\".format(pipeline_name))\n",
    "    except Exception as e:\n",
    "        print(\"{} \\n\".format(e))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KTCModelPackageGroupName model package group deleted\n",
      "KTCWorkflow pipeline deleted\n"
     ]
    }
   ],
   "source": [
    "delete_model_package_group(client, model_package_group_name)\n",
    "delete_sagemaker_pipeline(client, pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the Lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '58135353-588a-41b9-987d-c3d52fd2708f',\n",
       "  'HTTPStatusCode': 204,\n",
       "  'HTTPHeaders': {'date': 'Wed, 09 Nov 2022 11:44:21 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '58135353-588a-41b9-987d-c3d52fd2708f'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deploy_model_lambda_function.delete()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.6.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "393b72a4b6ebdc985529acd6cde114b0d5d1de410afe38058a3b12aa44b2c79b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
